{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "//anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:382: FutureWarning:\n",
      "\n",
      "set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [20/Mar/2020 14:31:23] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [20/Mar/2020 14:31:23] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [20/Mar/2020 14:31:23] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [20/Mar/2020 14:31:23] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "import plotly.express as px\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "# import inspect\n",
    "\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "from dash.dependencies import Input, Output\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from jupyter_plotly_dash import JupyterDash\n",
    "from importlib import reload\n",
    "\n",
    "# import functions as fun\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "rotten_df_cut = pd.read_csv('rotten_df_dash.csv', index_col=0)\n",
    "screenplays_cut = pd.read_csv('screenplays_dash.csv', index_col=0)\n",
    "\n",
    "# Function to create the example histograms\n",
    "\n",
    "def insert_hists(POS_label, title):\n",
    "    \n",
    "    # Separating out the good from the bad.\n",
    "    data1 = screenplays_cut[screenplays_cut.good_or_bad == 1]\\\n",
    "    [POS_label]\n",
    "    \n",
    "    data0 = screenplays_cut[screenplays_cut.good_or_bad == 0]\\\n",
    "    [POS_label]\n",
    "    \n",
    "    # Creating a cutoff at 3 standard deviations so that the graphs don't get \n",
    "    # skewed to one side.\n",
    "    std_high_1 = data1.mean() \\\n",
    "        + data1.std()*3\n",
    "    std_low_1 = data1.mean() \\\n",
    "        - data1.std()*3\n",
    "    \n",
    "    plot_info_1 = data1.drop(data1[lambda x: x > std_high_1].index)\n",
    "    plot_info_1 = plot_info_1.drop(plot_info_1[lambda x: x < std_low_1].index)\n",
    "    \n",
    "    # Same thing for the bad scripts.\n",
    "    std_high_0 = data0.mean() \\\n",
    "        + data0.std()*3\n",
    "    std_low_0 = data0.mean() \\\n",
    "        - data0.std()*3\n",
    "    \n",
    "    plot_info_0 = data0.drop(data0[lambda x: x > std_high_0].index)\n",
    "    plot_info_0 = plot_info_0.drop(plot_info_0[lambda x: x < std_low_0].index)\n",
    "    \n",
    "    # I need to create different bin sizes for various plots, since there is\n",
    "    # such a variety of scales here.\n",
    "    bin_size=0\n",
    "    \n",
    "    if plot_info_1.mean() > 500:\n",
    "        bin_size = 100\n",
    "    elif plot_info_1.mean() > 1:\n",
    "        bin_size = .1\n",
    "    elif plot_info_1.mean() > .1:\n",
    "        bin_size = .005\n",
    "    elif plot_info_1.mean() > .01:\n",
    "        bin_size = .001\n",
    "    else:\n",
    "        bin_size = .0004\n",
    "        \n",
    "    x_title = 'Ratio to Total Words'\n",
    "        \n",
    "    if POS_label == 'word_count' or POS_label == 'unique_words':\n",
    "        x_title = 'Words'\n",
    "    elif POS_label.find('sentiment') >=0:\n",
    "        x_title = 'Standardized Score (-1 to 1)'\n",
    "    elif POS_label.find('sentence') >= 0:\n",
    "        x_title = 'Average Length'\n",
    "    else:\n",
    "        'Ratio to Total Words'      \n",
    "        \n",
    "    fig =  ff.create_distplot([plot_info_1, plot_info_0], \n",
    "                              bin_size=bin_size,\n",
    "                              group_labels=['Awesome Films', 'Awful Films'],\n",
    "                              colors=['rgb(255,20,20)', 'rgb(35,230,90)'],\n",
    "                              )\n",
    "    fig.layout.update(title=title,\n",
    "                      yaxis={'title':'Script Count'},\n",
    "                      xaxis={'title':x_title})\n",
    "    \n",
    "    return dcc.Graph(figure=fig)\n",
    "\n",
    "# def top_words(words, max_features, min_df, max_df):\n",
    "#     \"\"\"Takes in a series of documents and returns an ordered list of \n",
    "#     how frequently words appear as calculated by sum vs count.\n",
    "    \n",
    "#     Parameters:\n",
    "    \n",
    "#         words: Series\n",
    "#             A series of documents with words to be counted.\n",
    "            \n",
    "#         max_features: int\n",
    "#             Populates max_features value in vectorizer. Ceiling for how many\n",
    "#             words to use.\n",
    "            \n",
    "#         min_df: float or int\n",
    "#             Populates min_df value in vectorizer. Minimum documents a word\n",
    "#             must appear in to be counted.\n",
    "            \n",
    "#         max_df: float or int\n",
    "#             Populates max_df value in vectorizer. Maximum documents a word\n",
    "#             must appear in to be counted.\n",
    "        \n",
    "#     returns: \n",
    "#         List of tuples with word and ratio calculated by sum / count:\n",
    "#         (word, ratio), sorted by ratio.\"\"\"\n",
    "    \n",
    "#     # Initialize vectorizor and fit\n",
    "#     victor = CountVectorizer(max_features=max_features, \n",
    "#                              min_df=min_df, max_df=max_df)\n",
    "#     movies_victor = victor.fit_transform(words)\n",
    "    \n",
    "#     # Transform into SparceDataFrame.\n",
    "#     sdf = pd.SparseDataFrame(movies_victor, \n",
    "#                                      columns=victor.get_feature_names())\n",
    "    \n",
    "#     sdf.fillna(0, inplace=True)\n",
    "    \n",
    "#     # Ave_word_count will house the tuples data to be sorted.\n",
    "#     ave_word_count = []\n",
    "#     for col in sdf.columns:\n",
    "#         key = col\n",
    "        \n",
    "#         # Calculate the ratio and add tuple to list.\n",
    "#         value = sum(sdf[col]) / len(sdf[col])\n",
    "#         ave_word_count.append((key, value))\n",
    "        \n",
    "#     # Return sorted tuple with word and ratio.\n",
    "#     return sorted(ave_word_count, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Visuals\n",
    "\n",
    "# # These visuals will be assembled for the purpose of the project dashboard, which will be run on an html page, ultimately to be hosted publicly. \n",
    "\n",
    "# ## Word Clouds\n",
    "\n",
    "# # Separating out the good movies from the bad and finding the most frequently used words.\n",
    "\n",
    "# # screenplays_cut.head()\n",
    "\n",
    "# # Creating dataframes for good and bad movies, then getting the top words.\n",
    "# good_movies_nostop = screenplays_cut[\n",
    "#     screenplays_cut.good_or_bad == 1]['no_stop']\n",
    "\n",
    "# good_top_words = top_words(words=good_movies_nostop, max_features=5000, \n",
    "#           min_df=.2, max_df=1.0)\n",
    "\n",
    "# bad_movies_nostop = screenplays_cut[\n",
    "#     screenplays_cut.good_or_bad == 0]['no_stop']\n",
    "\n",
    "# bad_top_words = top_words(words=bad_movies_nostop, max_features=5000, \n",
    "#           min_df=.2, max_df=1.0)\n",
    "\n",
    "# # Creating list to separate words from tuples\n",
    "# just_bad_words = [wordpair[0] for wordpair in bad_top_words]\n",
    "\n",
    "# good_not_bad = [(wordpair[0], wordpair[1]) for wordpair in good_top_words\n",
    "#                 if wordpair[0] not in just_bad_words]\n",
    "\n",
    "# good_not_bad\n",
    "\n",
    "# # I'm going to get the top words from bad and good that are exclusive from the other side. So, only the top good words than are not in the bad films, and for the bad film list, only the top words that are not in the good film list.\n",
    "\n",
    "# # Creating list to separate words from tuples\n",
    "# just_good_words = [wordpair[0] for wordpair in good_top_words]\n",
    "\n",
    "# bad_not_good = [(wordpair[0], wordpair[1]) for wordpair in bad_top_words\n",
    "#                 if wordpair[0] not in just_good_words]\n",
    "\n",
    "# bad_not_good\n",
    "\n",
    "# # To get this information into a word cloud, I'm actually going to de-tokenize the words and put them all in a string.\n",
    "\n",
    "# good_string = ''\n",
    "\n",
    "# for wordpair in good_not_bad:\n",
    "#     mul = wordpair[1] * 20\n",
    "#     word_rep = (wordpair[0] + ' the ')*int(mul)\n",
    "#     good_string += word_rep\n",
    "\n",
    "# # Using image masks found from http://www.clker.com/.\n",
    "\n",
    "# # thumbs_down_mask = np.array(Image.open('images/thumbs-dwn-icon-black-th.svg.hi.png'))\n",
    "# # thumbs_up_mask = np.array(Image.open('images/thumbs-up-icon-black-hi.png'))\n",
    "\n",
    "# # wordcloud = WordCloud(width=800, height=800, \n",
    "# #                       min_font_size=10,\n",
    "# #                       background_color='white',\n",
    "# #                       collocations=False,\n",
    "# #                       mask=thumbs_up_mask,\n",
    "# #                       contour_width=1,\n",
    "# #                       contour_color='yellow').generate(good_string)\n",
    "\n",
    "# # plt.figure(figsize=(8, 8), facecolor=None)\n",
    "# # plt.imshow(wordcloud)\n",
    "# # plt.axis('off')\n",
    "# # plt.tight_layout(pad=0)\n",
    "# # plt.savefig('images/good_cloud.png')\n",
    "# # plt.show()\n",
    "\n",
    "# # Same code used for the thumbs down word cloud for the bad films.\n",
    "\n",
    "# bad_string = ''\n",
    "\n",
    "# for wordpair in bad_not_good:\n",
    "#     # 20 is an arbitrary number; just need something to break them apart.\n",
    "#     mul = wordpair[1] * 20\n",
    "#     word_rep = (wordpair[0] + ' the ')*int(mul)\n",
    "#     bad_string += word_rep\n",
    "\n",
    "# wordcloud = WordCloud(width=800, height=800, \n",
    "#                       min_font_size=10,\n",
    "#                       background_color='white',\n",
    "#                       collocations=False,\n",
    "#                       mask=thumbs_down_mask,\n",
    "#                       contour_width=1,\n",
    "#                       contour_color='yellow').generate(bad_string)\n",
    "\n",
    "# plt.figure(figsize=(8, 8), facecolor=None)\n",
    "# plt.imshow(wordcloud)\n",
    "# plt.axis('off')\n",
    "# plt.tight_layout(pad=0)\n",
    "# plt.savefig('images/bad_cloud.png')\n",
    "# plt.show()\n",
    "\n",
    "# Some definite patterns emerge.\n",
    "\n",
    "# Dashboard\n",
    "\n",
    "## POS Histogram\n",
    "\n",
    "# For the histogram/distplot dropdown, I'll need to create materials to tell the dashboard what to show and options to choose from.\n",
    "\n",
    "# Note that the actual code for the graph has been moved below so that it can appear in the dash callback.\n",
    "\n",
    "POS_abb = screenplays_cut.columns[list(screenplays_cut.columns).index(\n",
    "    'word_count'):\n",
    "                                  list(screenplays_cut.columns).index(\n",
    "                                      'sentence_length')+1]\n",
    "POS_abb = POS_abb.append(screenplays_cut.columns[list(screenplays_cut.columns).index(\n",
    "    'PROPN'):\n",
    "                                   list(screenplays_cut.columns).index(\n",
    "                                       'PRON')+1])\n",
    "\n",
    "# Removing items that just weren't helpful.\n",
    "POS_abb = list(POS_abb)\n",
    "POS_abb.remove('sentiment_scores')\n",
    "POS_abb.remove('sentiment_compound')\n",
    "POS_abb.remove('colon_ratios')\n",
    "POS_abb.remove('semi_ratios')\n",
    "POS_abb.remove('SYM')\n",
    "POS_abb.remove('X')\n",
    "POS_abb.remove('SPACE')\n",
    "\n",
    "POS_abb\n",
    "\n",
    "# List of items to include in the dropdown. Even though I refer to the list\n",
    "# as POS, there are a few more attributes than just parts of speech.\n",
    "POS_desc = ['WORD COUNT', 'UNIQUE WORDS', 'NEGATIVE SENTIMENT', \n",
    "            'NEUTRAL SENTIMENT', 'POSITIVE SENTIMENT',\n",
    "            'COMMAS', 'ELLIPSES', 'SENTENCE LENGTH',\n",
    "            'PROPER NOUN', 'PUNCTUATION', 'VERB',\n",
    "            'ADJECTIVE', 'ADPOSITION', 'ADVERB', \n",
    "            'AUXILLIARY', 'COORDINATING CONJUNCTION', 'DETERMINER',\n",
    "            'INTERJECTION', 'NOUN', 'NUMERICAL', 'PARTICIPLE',\n",
    "            'PRONOUN']\n",
    "\n",
    "# This will marry the label (viewer-facing) to the value (data-facing).\n",
    "POS_hist_dict = dict(zip(POS_abb, POS_desc))\n",
    "\n",
    "# This will Put everything into its final format needed by dash.\n",
    "POS_hist_selector_list = []\n",
    "\n",
    "for k,v in POS_hist_dict.items():\n",
    "    temp = dict()\n",
    "    temp['label'] = v\n",
    "    temp['value'] = k\n",
    "    POS_hist_selector_list.append(temp)\n",
    "\n",
    "POS_hist_selector_list\n",
    "\n",
    "## Correlation Graph\n",
    "\n",
    "# Using XG Boost model to create feature importances to graph later on.\n",
    "\n",
    "# The usual classification setup. I won't need test sets since all I'm looking\n",
    "# for is the feature importance, and not the actual scoring or predictions.\n",
    "\n",
    "# X_train = screenplays_cut.no_stop\n",
    "# y_train = screenplays_cut.good_or_bad\n",
    "\n",
    "# # This will be the TFIDF version of the classification, since it was the most\n",
    "# # accurate.\n",
    "# tfidf = TfidfVectorizer(max_df=.95, min_df=.1, max_features=5000,\n",
    "#                          ngram_range=(1,2))\n",
    "# X2 = tfidf.fit_transform(X_train)\n",
    "\n",
    "# # There a few more bad scripts than good ones, so I'll make them even.\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_resampled, y_resampled = rus.fit_resample(pd.DataFrame(X2), y_train)\n",
    "\n",
    "# clf = XGBClassifier(max_depth=8,\n",
    "#                     criterion='entropy',\n",
    "#                     min_samples_split=14,\n",
    "#                     min_samples_leaf=1,\n",
    "#                     max_features=160)\n",
    "\n",
    "# clf.fit(X2, y_train)\n",
    "\n",
    "# columns = tfidf.get_feature_names()\n",
    "\n",
    "# Printing and plotting.\n",
    "# print(pd.Series(clf.feature_importances_,\n",
    "#               index=columns).sort_values(ascending=False).head(15))\n",
    "\n",
    "\n",
    "# df_importance = pd.Series(clf.feature_importances_, \n",
    "#                           index=columns)\n",
    "# df_importance = pd.DataFrame(df_importance)\n",
    "# df_importance.to_csv('df_importance.csv')\n",
    "df_importance = pd.read_csv('df_importance.csv', index_col=0)\n",
    "df_importance = df_importance['0']\n",
    "\n",
    "\n",
    "# # df_importance.plot(kind='barh', figsize=(8,15))\n",
    "# # plt.title('Most Important Features')\n",
    "# # plt.ylabel('Feature Name')\n",
    "# # plt.xlabel('Feature Importance')\n",
    "# # plt.show()\n",
    "\n",
    "# # Once I have the impoirtant features, I'll sort them and marry them up to a correlation matrix so that I can show how much the goood and bad movies are correlated to each top feature (word).\n",
    "\n",
    "# df_importance.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# important_df = pd.SparseDataFrame(X2, columns=tfidf.get_feature_names(),\n",
    "#                                default_fill_value=0 )\n",
    "\n",
    "# important_df = important_df[list(df_importance.index)]\n",
    "# important_df['good_or_bad'] = y_train\n",
    "\n",
    "# important_corr = important_df.corr()\n",
    "\n",
    "# important_corr.to_csv('important_corr.csv')\n",
    "important_corr = pd.read_csv('important_corr.csv', index_col=0)\n",
    "\n",
    "# I have to get rid of the rating column itself since it's not part of the \n",
    "# actual word list.\n",
    "feature_corr = important_corr.good_or_bad.drop('good_or_bad', axis=0)\n",
    "\n",
    "# Then separate out the good and the bad.\n",
    "good_corr = important_corr[important_corr['good_or_bad'] > 0]['good_or_bad']\n",
    "bad_corr = important_corr[important_corr['good_or_bad'] <= 0]['good_or_bad']\n",
    "\n",
    "# Putting it all together\n",
    "for i,v in bad_corr.items():\n",
    "    value = v + abs(v*2)\n",
    "    bad_corr.set_value(i,value) \n",
    "features_for_graph = pd.DataFrame([feature_corr, good_corr, bad_corr], \n",
    "                                  columns=['features', 'awesome', 'awful'])\n",
    "\n",
    "# Creating the information that the dash will need to toggle the 3 bar graphs.\n",
    "\n",
    "features_for_graph = {'features':df_importance, 'awesome':good_corr,\n",
    "                          'awful':bad_corr}\n",
    "\n",
    "features_for_graph = pd.DataFrame.from_dict(features_for_graph)\n",
    "#     .drop('good_or_bad', axis=0)\n",
    "\n",
    "# Carving out the class column.\n",
    "# good_corr = good_corr.drop('good_or_bad')\n",
    "# bad_corr = bad_corr.drop('good_or_bad')\n",
    "\n",
    "# Test graphic\n",
    "\n",
    "# data = go.Bar(x=bad_corr, y=bad_corr.index, orientation='h', \n",
    "#               marker={'color':'rgba(250, 250, 0, .5)',\n",
    "#                                  'line':{'color':'rgba(0,0,0,1)',\n",
    "#                                          'width':1}})\n",
    "# layout = go.Layout(width=400, height=600, yaxis={'autorange':\"reversed\"})\n",
    "\n",
    "# fig = go.Figure(data, layout)\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "# Creating the bar charts that will show the top 25 words of their respective type.\n",
    "\n",
    "# Need to wipe out a few naughty words.\n",
    "df_importance = df_importance[:25]\n",
    "indi = list(df_importance.index)\n",
    "for index, word in enumerate(indi):\n",
    "    if word.find('fuck') >= 0:\n",
    "        indi[index] = word.replace('fuck','f**k')\n",
    "df_importance.index = indi\n",
    "\n",
    "good_corr = good_corr[:25]\n",
    "indi = list(good_corr.index)\n",
    "for index, word in enumerate(indi):\n",
    "    if word.find('fuck') >= 0:\n",
    "        indi[index] = word.replace('fuck','f**k')\n",
    "good_corr.index = indi\n",
    "\n",
    "bad_corr = bad_corr[:25]\n",
    "indi = list(bad_corr.index)\n",
    "for index, word in enumerate(indi):\n",
    "    if word.find('fuck') >= 0:\n",
    "        indi[index] = word.replace('fuck','f**k')\n",
    "bad_corr.index = indi\n",
    "\n",
    "# for index, word in df_importance.items():\n",
    "#     print(index)\n",
    "#     if index.find('fuck') >= 0:\n",
    "#         df_importance.index[index] = 'f**k'\n",
    "\n",
    "# These are for the html.radio element.\n",
    "feature_selector_dicts = [{'label': 'Word Importance', 'value': 'df_importance'},\n",
    "                          {'label': 'Awesome Film Words', 'value': 'good_corr'},\n",
    "                          {'label': 'Awful Film Words', 'value': 'bad_corr'}]\n",
    "\n",
    "# These are to line up the data inside the function.\n",
    "feature_selector_data = {'df_importance': df_importance, 'good_corr': good_corr,\n",
    "                         'bad_corr': bad_corr}\n",
    "\n",
    "# This will assign different colors to the different graphs.\n",
    "feature_color_dicts = {'df_importance': 'rgba(250, 250, 0, .5)',\n",
    "                       'good_corr': 'rgba(35,220,90,.75)',\n",
    "                       'bad_corr': 'rgba(35,0,75,.5)'}\n",
    "\n",
    "# type(good_corr)\n",
    "\n",
    "## Unsupervised Category Predictions\n",
    "\n",
    "# In the EDA section, I used latent derichlet allocation to draw categories from the text and group the movies by those categories. I used ten categories because that's what worked well for modeling, but any number could actually be specified. I'll be showing them on the dashboard now, and allowing the user to select between the category to see what movies are in them.\n",
    "\n",
    "# Bringing in the data from the other notebook that I'll need for this.\n",
    "combined_df = pd.read_csv('combined_df.csv', index_col=0)\n",
    "cat_word_df = pd.read_csv('cat_word_df.csv', index_col=0)\n",
    "\n",
    "# Since these need to be in a different format from the work I've already done\n",
    "# on the lda categories, I simply re-pasted them into these dictionaries.\n",
    "lda_cats = [{'label':'Dark & Political', 'value':0,},\n",
    "            {'label':'Sports, Comedy, Silly Horror', 'value':1},\n",
    "            {'label':'Conflict', 'value':2},\n",
    "            {'label':'Holiday, Films I Haven\\'t Seen', 'value':3},\n",
    "            {'label':'Light-Hearted', 'value':4},\n",
    "            {'label':'Unusual Language or Slang', 'value':5},\n",
    "            {'label':'Violence & Gangster', 'value':6},\n",
    "            {'label':'Romance & Light Drama', 'value':7},\n",
    "            {'label':'Life Stories', 'value':8},\n",
    "            {'label':'Straight Up Horror', 'value':9}]\n",
    "\n",
    "for col in cat_word_df.columns:\n",
    "    cat_word_df[col] = cat_word_df[col].str.replace('fuck', 'f**k')\n",
    "    cat_word_df[col] = cat_word_df[col].str.replace('bitch', 'b**ch')\n",
    "    cat_word_df[col] = cat_word_df[col].str.replace('ass', 'a**')\n",
    "\n",
    "\n",
    "\n",
    "## Dashboard Text\n",
    "\n",
    "# This is all the text that will be used in the body of the dashboard itself.\n",
    "\n",
    "byline = ['Terry Ollila', html.Br(), 'terryollila@gmail.com',\n",
    "          html.Br(), 'March 18, 2020']\n",
    "\n",
    "intro_text = \"\"\"The film industry worldwide does upwards of 50 billion dollars \n",
    "    in box office sales, not counting home entertainment revenue, which brings \n",
    "    it up closer to 150 billion dollars. Operating within that bundle of cash \n",
    "    comes with a tremendous amount of risk, with major studios \n",
    "    sometimes spending a quarter of a billion dollars or more on a single film. \n",
    "    Decisions made at smaller studios are no less important to them, as they \n",
    "    might be putting their entire livelihoods on the line in the hopes of a hit. \n",
    "    And the value of a movie begins with a script. Knowing what might be a\n",
    "    potential money maker, and what might tank the bank, is an essential part\n",
    "    of the business. Natural language progression and supervised machine learning \n",
    "    classification models can use the text of a script to predict if a screenplay\n",
    "    is more likely or less likely to support a highly rated movie.\"\"\"\n",
    "\n",
    "intro_text_2 = \"\"\"Below is a look at some of the many factors that might \n",
    "    separate a highly-rated screenplay from one that's been critically panned.\n",
    "    Over 4,000 screenplays were dissected to create the below results. \n",
    "    Scripts were first broken apart and analyzed using their word counts,\n",
    "    sentiment scores, sentence lengths, and parts of speech. Following that,\n",
    "    they went through TFIDF vectorization as a means of analyzing the texts\n",
    "    word by word and assigning them importance. All of\n",
    "    this information was then used in classification modeling, most notably \n",
    "    using support vector machines and random forest classifiers, among \n",
    "    numerous others.\"\"\"\n",
    "\n",
    "thumb_text = \"\"\"Here is a breakout of the most common words among highly \n",
    "    rated films and lowly rated films. Only words that are unique were chosen,\n",
    "    so that for example, the word 'dude', while prominent in bad films, appeared \n",
    "    rarely if at all in good ones. Before you ask, I'm assuming The Big \n",
    "    Lebowski was not included in the sample.\"\"\"\n",
    "\n",
    "thumb_text_2 = \"\"\"If you're attempting to write a screenplay and you find \n",
    "    yourself using a lot of the words on the right, you might think about \n",
    "    taking another writing class or two before continuing on. Either that,\n",
    "    or set aside your idea for Cop Dude Killer for a day when the public is more \n",
    "    ready for it.\"\"\"\n",
    "\n",
    "hist_text = \"\"\"Moving on to breaking up the scripts into their component parts, \n",
    "    below are distribution plots showing various \n",
    "    attributes such as word count and sentence length, as well as parts of \n",
    "    speech. Here is an example showing the \n",
    "    distribution of auxilliary verbs.\"\"\"\n",
    "\n",
    "hist_text_2 = \"\"\"The good scripts trend toward higher numbers of auxilliary\n",
    "    verbs, while the bad ones cluster around one smaller area.\"\"\"\n",
    "\n",
    "hist_text_3 = \"\"\"Below is a similar graph, this time showing the distrubtion \n",
    "    of interjections trending higher for the lesser screenplays.\"\"\"\n",
    "\n",
    "hist_text_4 = \"\"\"The differences for each may seem slight, but in accumulation during \n",
    "    modeling, they can assist with overall prediction.\"\"\"\n",
    "\n",
    "hist_text_5 = \"\"\"All of these attributes can be used when creating predictive models \n",
    "    to predicit if a screenplay will be well-received or not. The dropdown\n",
    "    below selects between 22 different screenplay attributes, including various parts\n",
    "    of speech, sentiment, word counts, and sentence length.\"\"\"\n",
    "\n",
    "importance_text = \"\"\"Over to the left you'll see the words that featured\n",
    "    most prominently in creating a predictive model to determine if a \n",
    "    movie would be rated high or low. The initial view indicates how significant\n",
    "    those words were in making that prediction, with the most important words\n",
    "    listed higher and on down in descending order. These are then broken out\n",
    "    by good movies and bad movies, with the length of the bars representing \n",
    "    the correlation of that word to the screenplays, still in descending\n",
    "    order of importance in making predictions.\"\"\"\n",
    "\n",
    "category_text_dot5 = \"\"\"In natural language processing, the latent Derichlet \n",
    "    allocation provides a mathmatical means of clustering texts together by\n",
    "    means of their linguistic contents.\"\"\"\n",
    "\n",
    "category_text = \"\"\"In this case, thousands of screenplays from metacritic and \n",
    "    rottentomatoes were grouped according to their linguistic characteristics.\n",
    "    Sometimes, these lined up with a particular genre, and sometimes not.\n",
    "    After reviewing the titles, I labeled the categories as something that\n",
    "    I thought best captured the bulk of the movies, but it wasn't always a \n",
    "    perfect grouping, noting to the right the selecting of Baby Geniuses among\n",
    "    films such as The Murder Of Nicole Brown Simpson and Feardotcom.\n",
    "    Or is it so different after all? Something brought them together here...\"\"\"\n",
    "\n",
    "summary_text = \"\"\"Using the above attributes and more, I was able to create\n",
    "    a predictive models that would ascertain whether a script was from a \n",
    "    top-rated movie or a bottom-rated movie about 65% of the time. Within\n",
    "    that models was some flexibility, however. Some models were better at \n",
    "    predicting whether a movie would be 'good' (a true positive), while others\n",
    "    were better at predicting whether a movie would be 'bad' (a true negative). \n",
    "    The purpose of the prediction would dictate which model to employ.\"\"\"\n",
    "\n",
    "summary_recommendations_head = html.P(\"\"\"Some recommendations on what can be \n",
    "    done with this information: \"\"\")\n",
    "\n",
    "summary_recommendations = html.Li(children=[\"\"\"Studios looking to cull out \n",
    "    poor scripts from their stock should use the TFIDF with neural network\n",
    "    model, which has an 81% rating for correctly predicting a bad scripts,\n",
    "    and a 64% accuracy rating overall.\"\"\"])\n",
    "    \n",
    "summary_recommendations_2 = html.Li(children=[\"\"\"\n",
    "    Filmmakers looking create the largest set of good scripts from the total\n",
    "    should use the random boost with script attributes model, \n",
    "    which has a 69% chance of correctly predicting a good script,\n",
    "    though only a 55% accuracy score overall.\"\"\"])\n",
    "\n",
    "summary_recommendations_3 = html.Li(children=[\"\"\"\n",
    "    Filmmakers and screenwriters looking to find the most balanced filter\n",
    "    for finding good screenplays and culling out the bad should look to the\n",
    "    stacked model with support vector classifier using TFIDF followed by \n",
    "    a random forest classifier using script attributes, which had the \n",
    "    highest overall accuracy rate of 68%. If you're a screenwriter seeing \n",
    "    your work coming up on the bad side, and especially if it contains a \n",
    "    number of the \"thumbs down\" words noted above, you might want to start \n",
    "    rethinking your life choices.\"\"\", \n",
    "    html.Br(), \n",
    "    html.Br(),                                            \n",
    "    \"\"\"This last, most overall accurate and balanced model is represented in the \n",
    "    below matrix, where 1 represents the good scripts and 0 represents the bad\n",
    "    scripts, putting true positive (goods scripts correctly predicted to be good) on \n",
    "    the bottom right, and true negative (bad scripts correctly predicted to be bad) on\n",
    "    the top left.\"\"\"])\n",
    "\n",
    "summary_future_head = html.P(\"\"\"The scope of this idea is large, and \n",
    "    there are numerous opportunities for further study. A few I would like to\n",
    "    peruse are: \"\"\")\n",
    "\n",
    "summary_future = html.Li(\n",
    "    [\"\"\"Using TFIDF on the word clouds to get \n",
    "    a different, possibly more accurate take on what words are realistically\n",
    "    more prevalent throughout a large body of texts.\"\"\"])\n",
    "\n",
    "summary_future_2 = html.Li([ \n",
    "    \"\"\"Continue modeling with neural networks, especially with GloVe or other\n",
    "    existing word embedding libraries.\"\"\"])\n",
    "\n",
    "summary_future_3 = html.Li([\n",
    "    \"\"\"Continue further work on regression analysis to predict  no just \n",
    "    whether a scripts is good or bad, but to predict the actual score it will\n",
    "    receive on a scale of 0 to 100.\"\"\"])\n",
    "\n",
    "summary_future_4 = html.Li([\n",
    "    \"\"\"In the dashboard, provide a text entry or upload option where a script\n",
    "    could be entered and a 'good' or 'bad' rating could be assigned, to \n",
    "    easily determine where a given script might lie.\"\"\"])\n",
    "\n",
    "final_text = \"\"\"Hopefully you've found this display informative, and have \n",
    "    gained some\n",
    "    furthered understanding of how a film critics love might differ from \n",
    "    one they hate at the writing's most fundamental level. Writers probably \n",
    "    won't find much luck cramming their screenplays full of auxilliary verbs \n",
    "    and increasing their word counts in hopes of improving their chances of \n",
    "    penning a hit, but it is information that can be used to cull the good from\n",
    "    the bad before a major financial mistake is made\"\"\"\n",
    "\n",
    "thanks = \"\"\"Thanks!\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "## Main Dash Code\n",
    "\n",
    "# This is the skeleton for the web page for the dashboard. Interactions will be managed by callbacks, and styles will be managed by a separate style sheet: main.css in the assets folder.\n",
    "\n",
    "# app = JupyterDash('POS_histogram')\n",
    "app = dash.Dash()\n",
    "server = app.server\n",
    "\n",
    "# Reading in the files for the word cloud images.\n",
    "image_filename = 'images/good_cloud.png'\n",
    "encoded_good_cloud = base64.b64encode(\n",
    "    open(image_filename, 'rb').read()).decode('ascii')\n",
    "\n",
    "image_filename = 'images/bad_cloud.png'\n",
    "encoded_bad_cloud = base64.b64encode(\n",
    "    open(image_filename, 'rb').read()).decode('ascii')\n",
    "\n",
    "# The main block of code generating the HTML. Using separate file for \n",
    "# CSS styling.\n",
    "app.layout = html.Div(children=[\n",
    "    html.H1(className='head',\n",
    "            children=['What\\'s a', html.Br(), 'Writer Worth?']\n",
    "           ),\n",
    "    html.H2(className='subHead',\n",
    "            children=[\"\"\"Screenplay Science and \n",
    "                the Value of a Few Good Words\"\"\"]\n",
    "           ),\n",
    "    html.Div(className='mainBox',\n",
    "             children=[\n",
    "        html.Div(className='subBox',\n",
    "                 children=[\n",
    "            # By Line\n",
    "            html.P(id='byline',\n",
    "                   children=byline),\n",
    "            # Introductory text         \n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(intro_text),\n",
    "                    html.P(intro_text_2)\n",
    "                ]),\n",
    "            html.H3('Unique Words'),\n",
    "                     \n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(thumb_text)\n",
    "                ]),\n",
    "            \n",
    "            # This is where the word clouds will go.\n",
    "                     \n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(thumb_text_2)\n",
    "                ]),\n",
    "            html.H3('Structure and Parts of Speech'),\n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(hist_text),\n",
    "                ]),                    \n",
    "                     \n",
    "            # This is where the histograms will go.   \n",
    "            html.Div(className='example_hist',\n",
    "                     children=[insert_hists('AUX', 'Auxilliary Verbs')]),\n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(hist_text_2),\n",
    "                ]),\n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(hist_text_3),\n",
    "                ]),          \n",
    "            html.Div(className='example_hist',\n",
    "                     children=[insert_hists('INTJ', 'Interjections')]),\n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(hist_text_4),\n",
    "                ]),\n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(hist_text_5),\n",
    "                ]),             \n",
    "            html.Div(id='hist', children=[\n",
    "                dcc.Dropdown(id='hist_selector',\n",
    "                         options=POS_hist_selector_list,\n",
    "                         value='word_count'),\n",
    "                dcc.Graph(\n",
    "                    id='hist_graph')\n",
    "                ]),\n",
    "                     \n",
    "            # This is where the bar charts for temp importance will go.\n",
    "                     \n",
    "            # This is where we will show the LDA category information.\n",
    "            html.H3('Unsupervised Category Creation'),\n",
    "                     \n",
    "            html.P(className='sidebar',\n",
    "                   children=[category_text_dot5]),\n",
    "            html.P(className='sidebar',\n",
    "                   children=[category_text]),\n",
    "            html.H3('It\\'s a Wrap'),\n",
    "            html.P(className='sidebar',\n",
    "                   children=[summary_text]),\n",
    "            html.P(className='sidebar',\n",
    "                   children=[summary_recommendations_head]),\n",
    "            html.Ul(className='sidebar',\n",
    "                   children=[summary_recommendations,\n",
    "                             summary_recommendations_2,\n",
    "                             summary_recommendations_3\n",
    "                            ]),\n",
    "            html.Img(className='confused',\n",
    "                   src='assets/conf_matrix.png'),\n",
    "            html.P(className='sidebar',\n",
    "                   children=[summary_future_head]),\n",
    "            html.Ul(className='sidebar',\n",
    "                   children=[summary_future,\n",
    "                             summary_future_2,\n",
    "                             summary_future_3,\n",
    "                             summary_future_4]),\n",
    "            html.Div(\n",
    "                className='textBlock',\n",
    "                children=[\n",
    "                    html.P(final_text),\n",
    "                    html.P(thanks)]),\n",
    "            ]),\n",
    "        ]),         \n",
    "    ])\n",
    "\n",
    "## Callbacks\n",
    "\n",
    "# The distplot will select between a number of different script attributes.\n",
    "\n",
    "# Callback and function for POS graphs and switching dropdown.\n",
    "@app.callback(Output(component_id='hist_graph', \n",
    "                     component_property='figure'),\n",
    "              [Input(component_id='hist_selector',\n",
    "                     component_property='value')])\n",
    "def insert_hist(POS_label):\n",
    "    \n",
    "    # Separating out the good from the bad.\n",
    "    data1 = screenplays_cut[screenplays_cut.good_or_bad == 1]\\\n",
    "    [POS_label]\n",
    "    \n",
    "    data0 = screenplays_cut[screenplays_cut.good_or_bad == 0]\\\n",
    "    [POS_label]\n",
    "    \n",
    "    # Creating a cutoff at 3 standard deviations so that the graphs don't get \n",
    "    # skewed to one side.\n",
    "    std_high_1 = data1.mean() \\\n",
    "        + data1.std()*3\n",
    "    std_low_1 = data1.mean() \\\n",
    "        - data1.std()*3\n",
    "    \n",
    "    plot_info_1 = data1.drop(data1[lambda x: x > std_high_1].index)\n",
    "    plot_info_1 = plot_info_1.drop(plot_info_1[lambda x: x < std_low_1].index)\n",
    "    \n",
    "    # Same thing for the bad scripts.\n",
    "    std_high_0 = data0.mean() \\\n",
    "        + data0.std()*3\n",
    "    std_low_0 = data0.mean() \\\n",
    "        - data0.std()*3\n",
    "    \n",
    "    plot_info_0 = data0.drop(data0[lambda x: x > std_high_0].index)\n",
    "    plot_info_0 = plot_info_0.drop(plot_info_0[lambda x: x < std_low_0].index)\n",
    "    \n",
    "    # I need to create different bin sizes for various plots, since there is\n",
    "    # such a variety of scales here.\n",
    "    bin_size=0\n",
    "    \n",
    "    if plot_info_1.mean() > 500:\n",
    "        bin_size = 100\n",
    "    elif plot_info_1.mean() > 1:\n",
    "        bin_size = .1\n",
    "    elif plot_info_1.mean() > .1:\n",
    "        bin_size = .005\n",
    "    elif plot_info_1.mean() > .01:\n",
    "        bin_size = .001\n",
    "    else:\n",
    "        bin_size = .0004\n",
    "        \n",
    "    x_title = 'Ratio to Total Words'\n",
    "        \n",
    "    if POS_label == 'word_count' or POS_label == 'unique_words':\n",
    "        x_title = 'Words'\n",
    "    elif POS_label.find('sentiment') >=0:\n",
    "        x_title = 'Standardized Score (-1 to 1)'\n",
    "    elif POS_label.find('sentence') >= 0:\n",
    "        x_title = 'Average Length'\n",
    "    else:\n",
    "        'Ratio to Total Words'      \n",
    "        \n",
    "    fig =  ff.create_distplot([plot_info_1, plot_info_0], \n",
    "                              bin_size=bin_size,\n",
    "                              group_labels=['Awesome Films', 'Awful Films'],\n",
    "                              colors=['rgb(255,20,20)', 'rgb(35,230,90)'],\n",
    "                              )\n",
    "    fig.layout.update(yaxis={'title':'Script Count'},\n",
    "                      xaxis={'title':x_title})\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# This will create the important word bar graph that will toggle between most important words overall, good film word correlation, and bad film word correlation.\n",
    "\n",
    "#Callback and function for feature bar graph and radio buttons.\n",
    "# @app.callback(Output(component_id='feature_graph', \n",
    "#                      component_property='figure'),\n",
    "#               [Input(component_id='feature_radio',\n",
    "#                      component_property='value')])\n",
    "# def insert_feature(radio_label):\n",
    "#     bar_data = [\n",
    "#         go.Bar(\n",
    "#             x=feature_selector_data[radio_label], \n",
    "#             y=feature_selector_data[radio_label].index, \n",
    "#             orientation='h', \n",
    "#             marker={'color': feature_color_dicts[radio_label],\n",
    "#                      'line':{'color':'rgba(150,10,10,1)',\n",
    "#                              'width':1}})]\n",
    "#     toggle={'df_importance':'Important Overall',\n",
    "#             'good_corr':'Awesome Films',\n",
    "#             'bad_corr':'Awful Films'}\n",
    "    \n",
    "#     imp_corr = 'Importance' if radio_label == 'df_importance' else 'Correlation'\n",
    "\n",
    "#     bar_layout = go.Layout(width=400, height=700, \n",
    "#                            title=toggle[radio_label],\n",
    "#                            xaxis={'title':imp_corr},\n",
    "#                            yaxis={'autorange':\"reversed\",\n",
    "#                                   'title':'Significant Terms'})\n",
    "\n",
    "#     return {'data':bar_data, 'layout':bar_layout}\n",
    "\n",
    "# This will create the box for displaying titles associated with the categories created using LDA.\n",
    "\n",
    "# Callback and function for movies grouped by LDA categories.\n",
    "# @app.callback(Output(component_id='cat_box', \n",
    "#                      component_property='children'),\n",
    "#               [Input(component_id='cat_drop',\n",
    "#                      component_property='value')])\n",
    "# def insert_titles(cat_id):\n",
    "    \n",
    "#     movie_list = combined_df[combined_df.category == cat_id]['titles']\n",
    "    \n",
    "#     formatted_titles = []\n",
    "    \n",
    "#     # Formatting the titles to look better than the dash format I've been using.\n",
    "#     for title in movie_list:\n",
    "#         temp = title.title().replace('-', ' ')\n",
    "#         if title[-4:] == '-the':\n",
    "#             temp = 'The ' + temp[:-4]\n",
    "        \n",
    "#         formatted_titles.append(temp)\n",
    "    \n",
    "#     # Inserting a line break in every other line.\n",
    "#     for i in range(len(formatted_titles)):\n",
    "#         formatted_titles.insert(i*2, html.Br())\n",
    "    \n",
    "#     # The below is to remove the initial line break that appears to to i*0\n",
    "#     formatted_titles.pop(0)\n",
    "    \n",
    "#     return formatted_titles\n",
    "\n",
    "# This is a separate function for adding in the categories themselves.\n",
    "\n",
    "# Callback for the LDA categories themselves.\n",
    "# @app.callback(Output(component_id='cat_box_2',\n",
    "#                      component_property='children'),\n",
    "#              [Input(component_id='cat_drop',\n",
    "#                     component_property='value')])\n",
    "# def insert_cats(cat_id):\n",
    "    \n",
    "#     these_cats = list(cat_word_df[str(cat_id)])\n",
    "    \n",
    "#     for i in range(len(these_cats)):\n",
    "#         these_cats.insert(i*2, html.Br())   \n",
    "\n",
    "#     these_cats.insert(0, html.Br())\n",
    "#     these_cats.insert(0, 'Most Significant Words')\n",
    "#     these_cats.insert(0, html.Br())\n",
    "    \n",
    "#     return these_cats\n",
    "\n",
    "# Running the dash.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # app.run_server(debug=False)\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
